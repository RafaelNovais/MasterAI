{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://gist.github.com/RafaelNovais/2dbebb9ed6b54e18537b92f3c71aa38d#file-assignment2_mas_23113607-ipynb",
      "authorship_tag": "ABX9TyNwJuAJuc6liuFblxRLgVMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/assignment2_mas_23113607.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pnvtPJHXKo",
        "outputId": "fd1f534b-f63f-421e-db4c-6e73d12d24bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . . .\n",
            "H . . H .\n",
            ". . . . .\n",
            ". H . . .\n",
            ". . H . G\n",
            "\n",
            ". A . . .\n",
            "H . . H .\n",
            ". . . . .\n",
            ". H . . .\n",
            ". . H . G\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FrozenLake:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 5\n",
        "        self.start_state = (0, 0)\n",
        "        self.goal_state = (4, 4)\n",
        "        self.holes = [(1, 0), (1, 3), (3, 1), (4, 2)]\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.start_state\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_pos\n",
        "        if action == 'up':\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 'down':\n",
        "            x = min(self.grid_size - 1, x + 1)\n",
        "        elif action == 'left':\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 'right':\n",
        "            y = min(self.grid_size - 1, y + 1)\n",
        "\n",
        "        new_pos = (x, y)\n",
        "\n",
        "        #terminal states\n",
        "        if new_pos in self.holes:\n",
        "            reward = -5.0\n",
        "            done = True\n",
        "        elif new_pos == self.goal_state:\n",
        "            reward = +10.0\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1.0\n",
        "            done = False\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "        return new_pos, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
        "        x, y = self.agent_pos\n",
        "        grid[x][y] = 'A'\n",
        "        for hx, hy in self.holes:\n",
        "            grid[hx][hy] = 'H'\n",
        "        gx, gy = self.goal_state\n",
        "        grid[gx][gy] = 'G'\n",
        "        for row in grid:\n",
        "            print(' '.join(row))\n",
        "        print()\n",
        "\n",
        "#usage teste\n",
        "if __name__ == \"__main__\":\n",
        "    env = FrozenLake()\n",
        "    env.render()\n",
        "    next_state, reward, done = env.step('right')\n",
        "    env.render()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0.5, gamma=0.9, epsilon=0.1, epsilon_decay=None):\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.q_table = {}\n",
        "        for i in range(env.grid_size):\n",
        "            for j in range(env.grid_size):\n",
        "                self.q_table[(i, j)] = {a: 0.0 for a in env.actions}\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.env.actions)\n",
        "        else:\n",
        "            q_vals = self.q_table[state]\n",
        "            return max(q_vals, key=q_vals.get)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        predict = self.q_table[state][action]\n",
        "        target = reward\n",
        "        if next_state not in self.env.holes and next_state != self.env.goal_state:\n",
        "            target += self.gamma * max(self.q_table[next_state].values())\n",
        "        self.q_table[state][action] += self.alpha * (target - predict)\n",
        "\n",
        "    def decay_epsilon(self, episode, total_episodes):\n",
        "        if self.epsilon_decay:\n",
        "            self.epsilon = self.initial_epsilon * (1 - episode / total_episodes)\n",
        "\n",
        "    def train(self, episodes=10000):\n",
        "        rewards = []\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                self.learn(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            self.decay_epsilon(ep, episodes)\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "        return rewards\n",
        "\n",
        "    def get_max_q_table(self):\n",
        "        return {s: max(a.values()) for s, a in self.q_table.items()}\n"
      ],
      "metadata": {
        "id": "7LjCfqPvH1WS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def run_experiment(name, alpha, gamma, epsilon, epsilon_decay=False):\n",
        "    env = FrozenLake()\n",
        "    agent = QLearningAgent(\n",
        "        env,\n",
        "        alpha=alpha,\n",
        "        gamma=gamma,\n",
        "        epsilon=epsilon,\n",
        "        epsilon_decay=epsilon_decay\n",
        "    )\n",
        "    rewards = agent.train(episodes=10000)\n",
        "\n",
        "    # Save Q-values to Google Drive\n",
        "    qvalue_path = f\"/content/drive/MyDrive/AI/{name}_qvalues.txt\"\n",
        "    with open(qvalue_path, \"w\") as f:\n",
        "        for state, q_val in agent.get_max_q_table().items():\n",
        "            f.write(f\"{state}: {q_val:.2f}\\n\")\n",
        "\n",
        "    # Save learning curve plot to Google Drive\n",
        "    plt.plot(rewards)\n",
        "    plt.title(f\"Learning Curve: {name}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.savefig(f\"/content/drive/MyDrive/AI/{name}_learning_curve.png\")\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "# Run all three experiments\n",
        "run_experiment(\"basic\", alpha=0.5, gamma=0.9, epsilon=0.1)\n",
        "run_experiment(\"epsilon_decay\", alpha=0.5, gamma=0.9, epsilon=0.1, epsilon_decay=True)\n",
        "run_experiment(\"custom\", alpha=0.6, gamma=0.95, epsilon=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1LluWJKnIGxd",
        "outputId": "75737bc4-ce16-4f32-b3b9-705a4e39ac85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}