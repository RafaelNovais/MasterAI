{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgAx2UZwuWXn4jtB1kRMT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/DeepLearningMindMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classic ML\n",
        "\n",
        "1 - Classification  Predict a function/classe\n",
        "\n",
        "2 - Regression      Predict a number/specific value\n",
        "\n",
        "3 - Clustering      Predict or create a Group based in details or different details\n",
        "\n",
        "4 - Co-trainig  is a cluster with classification, you can group based an specific object/details\n",
        "\n",
        "5 - Relationship Discovery - find association like Chips and ketchup\n",
        "\n",
        "6 - Reinforcement Learning - Positive Reinforcement like rewards, Negative Reinforcement Punishment , the agent need explore the envirament\n",
        "\n",
        "\n",
        "\n",
        "**Ensembles: Basic Idea** AGRUPAMENTO/COMIBINAR DE PROCESSAMENTOS/Modelos PARA DIVERSIFICAR PROCESAR VARIAS VEZES E AGRUPAR OS RESULTADOS \"Muitos sao mais inteligentes que alguns\" - sing your data, construct multiple classifiers,Combine the decisions to arrive at final decision\n",
        "\n",
        "* Bagging - Bootstrap Aggregation, decrease the variance by generating multiple data sets of the same size as the original data set - Processa Independente/Paralelo e depois combina\n",
        "  * Simulates the existence of multiple data sets\n",
        "  * Bagging works by reducing the variance through averaging/voting over multiple classifiers created with different training subsets\n",
        "  * orks well if the classification algorithm is unstable\n",
        "\n",
        "* Boosting ‚Äì Uses subsets of the original data set to produce averagely performing models weighted towards ‚Äúharder‚Äù problems. It then ‚Äúboosts‚Äù the results through a vote. Processo em sequencia, depois reprocessa N vezes ate classificacao final\n",
        "  * Also uses voting/averaging\n",
        "  * Weights models according to performance\n",
        "  * Boosting generates a series of classifiers\n",
        "  * Their output is combined to produce a weighted vote\n",
        "\n",
        "* Random Forests\n",
        "  * CART is a Decision Tree classifier,Create a ‚Äòforest‚Äô of trees as an ensemble\n",
        "  * Works like bagging, except ‚Äì Decision tree is base learner\n",
        "\n",
        "* Stacking ‚Äì Combines the results from multiple heterogeneous classifiers to improve classification. A voting scheme is not applied, instead a linear alg is\n",
        "applied to make the classification - Processa Paralelo depois combina Meta-Modelo heterogeneous\n",
        "  * Stacking, a.k.a Stacked Generalisation, is a family of methods for heterogenous ‚Äì Classifiers are built using different classification algorithms\n",
        "  * Because of their different characteristics, stacking seeks to combine them in a way that is more sophisticated than a simple vote\n",
        "\n",
        "\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "Highly parallel computation\n",
        "\n",
        "  ‚Äì Weights: positive or negative\n",
        "\n",
        "  ‚Äì Activation: 0/1, soft threshold; nonlinear; differentiable Commonly: sigmoid function\n",
        "\n",
        "  ‚Äì Output: \"Squashed\" linear function of inpu\n",
        "\n",
        "\n",
        "**Fully Connected Feed-Forward Neural Network**\n",
        "\n",
        "* Also known as a Multi-Layer Perceptron\n",
        "* Simplest architecture, widely used\n",
        "* Neurons connected in layers\n",
        "    ‚Äì Family of functions parameterised by weights\n",
        "    ‚Äì No internal state\n",
        "* High-dimensional non-linear interpolation\n",
        "* Network is a distributed model of the data\n",
        "\n",
        "\n",
        "\n",
        "# Gradient Descent with Backpropagation\n",
        "\n",
        "* orward propagation\n",
        "* Backpropagation Step\n",
        "  * Output Layer\n",
        "  * Hidden Layers\n",
        "* Gradient Descent Update Step\n",
        "\n",
        "\n",
        "# Common Activation Functions\n",
        "\n",
        "* Logistic\n",
        "* Tanh\n",
        "* ReLU\n",
        "* Leaky ReLU\n",
        "\n",
        "\n",
        "* **For classification tasks, we will use the same average log loss cost function that we saw last week for logistic regression**\n",
        "\n",
        "*\n",
        "\n",
        "# Deep Neural Networks\n",
        "\n",
        "* NNs with more than a small number of hidden layers Varias Camadas de processamento\n",
        "  * Generally large in scale: many input nodes; many hidden layers; many nodes per hidden layer; large amounts of training data\n",
        "\n",
        "\n",
        "**High-Level Perspectives**\n",
        "* Practical perspective:\n",
        "  * Deep architectures achieve many of the same things as shallow ones, but more efficiently, particularly for perception tasks(vision/sound)\n",
        "  * Basic training techniques were not effective, so we needed to find ways of getting it to work\n",
        "* Connectionist programming perspective:\n",
        "  * Deep learning provides an integrated approach to feature engineering and learning, all within a connectionist architecture\n",
        "  * Nodes in early layers can be considered functions/subroutines that are re-used in later ones\n",
        "  * Convolutional NNs extend this idea further\n",
        "\n",
        "* Structured Data\n",
        "  * typically organised in a table\n",
        "* Unstructured Data\n",
        "  * Photos, Movies, Audio, Documents\n",
        "* Multi-Class Classification - Represent classes as 3 or + outputs\n",
        "  * The Softmax function replaces the standard Sigmoid function used in binary classification. It rescales the z values so that the ‡∑úùë¶ values sum to 1, as required for a probability distribution.\n",
        "  \n",
        "* Methods to Avoid Overfitting\n",
        "  * Data Augmentation - The best way to improve generalisation on a ML model is to train it with more data\n",
        "  * Early Stopping - In early stages of NN training, when it is far from converging, overfitting is never an issue, but it can become an issue as training proceeds, particularly if network is complex relative to dataset size\n",
        "  * L2 regularisation - The idea behind regularization is that we add an extra penalty term to the cost function to penalise more complex networks  Python: numpy.linag.norm\n",
        "\n",
        "\n",
        "# Training Algorithms\n",
        "* Mini-Batch Gradient Descent\n",
        " * Divide full dataset into mini batches\n",
        " * Loop over mini-batches, and do one iteration of the training algorithm on 1 mini-batch\n",
        "\n",
        "\n",
        "* Backprop with Momentum\n",
        "  * Each parameter should be able to change at a rate appropriate to itself, rather than there being one fixed learning rate\n",
        "  * The previous changes in parameter values should influence the current direction of updates\n",
        "  * To achieve this, for each parameter, compute an exponentially weighted moving average of previous gradients, and use that to update the parameter\n",
        "\n",
        "* RMSprop Root Means Square\n",
        "  * Another adaptive learning rate algorithm, proposed by Geoff Hinton, and strongly related to an earlier one called AdaGrad,\n",
        "  * For each parameter, keep a moving average of its squared gradient\n",
        "  * When updating, divide the current gradient by the square root of the average squared gradient\n",
        "\n",
        "* Adam Optimiser = Adaptive Moment Estimation\n",
        "  * Essentially, combine the ideas from Momentum and RMSprop: consider both velocity and acceleration terms\n",
        "\n",
        "* Convolutional Networks\n",
        "  * Building on previous idea, introduce a set of shared weights and biases for each field\n",
        "  * Usually have multiple convolutional layers in a CNN\n",
        "\n",
        "\n",
        "* Model Re-Use\n",
        "  * Pre-Training\n",
        "  * Transfer Learning\n",
        "\n",
        "* Convolutions ???\n",
        "* Convolutions on Multi-Channel Inputs ??\n",
        "* Multiple Feature Detectors\n",
        "\n",
        "\n",
        "**CNNs Incorporate Important Ideas that can Help Learning**\n",
        "* Sparse Connectivity\n",
        "  * Rather than having fully-connected layers, only some units in one layer\n",
        "  are derived from values of some units in previous layer\n",
        "  * Reduces number of weights, reducing overfitting risk, computational cost\n",
        "  and training demands\n",
        "* Parameter Sharing\n",
        "  * Rather than learning a separate parameter for\n",
        "  each connection, learn shared parameters that\n",
        "  represent specific operations\n",
        "  * Again, reduces number of weights\n",
        "* Equivariant Representations\n",
        "  * If you apply a transformation to input and then put\n",
        "  it through conv layer, equivalent to putting it through\n",
        "  conv layer and then applying the transformation\n",
        "  * For CNNs, true of translation operations specifically,\n",
        "  but not others such as rotations"
      ],
      "metadata": {
        "id": "Pci65G941JLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro! Aqui vai um **resumo detalhado por tema** com explica√ß√µes curtas e diretas, focado em te dar um **entendimento r√°pido** do conte√∫do das provas CT5145 (Deep Learning). Ao final, te passo um **plano de estudos com v√≠deos recomendados do YouTube**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ RESUMO DE TEMAS ‚Äì CT5145 (2021 a 2024)\n",
        "\n",
        "### üß† 1. Fundamentos de Redes Neurais\n",
        "https://www.youtube.com/watch?v=CqOfi41LfDw\n",
        "\n",
        "- **Forward Propagation:** C√°lculo da sa√≠da da rede usando pesos e fun√ß√µes de ativa√ß√£o.\n",
        "- **Backpropagation:** Algoritmo que ajusta os pesos usando o gradiente da fun√ß√£o de custo.\n",
        "  - *Epoch:* Passagem de todos os dados de treino pela rede\n",
        "  - *Batch Size:* Passagem de um numero N de registro antes de atualizar os pesos\n",
        "- **Fun√ß√µes de Ativa√ß√£o:**\n",
        "  - *ReLU:* retorna 0 se input < 0, sen√£o retorna o pr√≥prio input.\n",
        "  - *Leaky ReLU:* igual ao ReLU, mas permite um pequeno gradiente para inputs negativos.\n",
        "  - *Sigmoid / Logistic:* mapeia entrada para o intervalo [0,1]. Curva S\n",
        "  - *TANH Hiper Tang:* mapeia entrada para o intervalo [-1,1]. Curva S\n",
        "  - *Threshold:* Reta ou 0 ou 1 _|-\n",
        "  - *Softplus:*  mas suave que a ReLU curvada entre 0 e >0\n",
        "  - *Softmax:* para multiclasse.\n",
        "- **Inicializa√ß√£o de Pesos:** Necess√°rio para quebrar simetria ‚Äì pesos devem ser aleat√≥rios.\n",
        "- **Fun√ß√µes de Custo/Perda:** Medem o erro entre predi√ß√£o e r√≥tulo. Ex: .\n",
        "  - *MSE:* Media Square\n",
        "  - *MAE:* Meadia\n",
        "  - *Huber:* Adjust Lost Fuction\n",
        "  - *cross-entropy:* Classificacao Perda somente e calculada no valor correto.\n",
        "- **Regulariza√ß√£o L2:** Penaliza pesos grandes, ajuda a evitar overfitting.\n",
        "- **Early Stopping:** Para o treino quando a performance no conjunto de valida√ß√£o piora.\n",
        "- **Regra da Cadeia:** Chain Rule uniao de dados baseado em comuns  \n",
        "  - Numero da camisa / Peso\n",
        "  - Peso / Altura\n",
        "  - Numero da Camisa > Peso > Altura\n",
        "\n",
        "---\n",
        "\n",
        "### üß± 2. Arquiteturas Avan√ßadas\n",
        "- **CNN (Convolutional Neural Networks):** Ideais para imagens. Usam filtros para extrair padr√µes espaciais.\n",
        "- **Pooling:** Reduz dimensionalidade. Tipos: MaxPooling, AveragePooling.\n",
        "- **Residual Connections:** Atalhos que ajudam em redes profundas, facilitando o fluxo de gradiente.\n",
        "- **Autoencoder:** Comprime e reconstr√≥i dados. Bom para compress√£o e redu√ß√£o de ru√≠do.\n",
        "- **VAE (Variational Autoencoder):** Adiciona aleatoriedade. √ötil para gerar novos dados.\n",
        "- **GAN (Generative Adversarial Network):** Gerador + Discriminador lutando entre si para criar dados realistas.\n",
        "- **Transformers:** Usam *self-attention* para processar sequ√™ncias em paralelo.\n",
        "- **U-Net:** Arquitetura de segmenta√ß√£o com skip connections (muito usada em imagem m√©dica).\n",
        "- **YOLO:** Modelo de detec√ß√£o de objetos em tempo real.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è 3. Treinamento e Otimiza√ß√£o\n",
        "- **Gradient Descent:** Atualiza pesos para minimizar a fun√ß√£o de custo. Custo de Treinamento baseado em acertos comparando com erros.\n",
        "  - *Stochastic GD:* 1 amostra por vez.\n",
        "  - *Batch GD:* todo o dataset.\n",
        "  - *Mini-batch:* pequenos grupos ‚Äì o mais usado.\n",
        "- **Momentum:** Acelera o GD usando o hist√≥rico das atualiza√ß√µes.\n",
        "- **RMSProp:** Ajusta o passo (learning rate) individualmente por peso.\n",
        "- **Overfitting vs Underfitting:** Quando o modelo aprende demais ou de menos.\n",
        "- **Batch Normalization:** Normaliza as ativa√ß√µes para acelerar e estabilizar o treino.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ 4. Classifica√ß√£o, Regress√£o e Avalia√ß√£o\n",
        "- **Camada de sa√≠da:**\n",
        "  - *Sigmoid:* para classifica√ß√£o bin√°ria. Curva S\n",
        "  - *Softmax:* para multiclasse.\n",
        "  - *Linear:* para regress√£o.\n",
        "\n",
        "- **Ensembles:** Inteligecia Coletiva / Mutiplos Modelos Combinados\n",
        "  - *Bagging:* m√∫ltiplos modelos treinados com subconjuntos dos dados. Bolsa de Dados / Paralelo\n",
        "  - *Boosting:* modelos treinados sequencialmente, focando nos erros anteriores. Reforco / Sequencial aprendendo com o anterior.\n",
        "  - *Stacking:* combina sa√≠das de m√∫ltiplos modelos.\n",
        "- **Curvas de Treinamento:** ajudam a detectar overfitting observando desempenho em treino vs valida√ß√£o.\n",
        "- **Decision Boundary:** Fronteira que separa as classes previstas.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà 5. S√©ries Temporais e Dados Sequenciais\n",
        "- **RNN (Rede Recorrente):** Mant√©m um estado interno para capturar sequ√™ncia.\n",
        "- **LSTM:** Redes de longa memoria, Pois em um texto com muitas palavras uma palavra do final pode ser relacionada com uma palavra no inicio e ira perder Correcao de pesos.  Resolve o problema do \"desvanecimento de gradiente\" em RNNs longas.\n",
        "- **Teacher Forcing:** Durante treino, o modelo usa a resposta correta anterior como entrada.\n",
        "- **Time-series Forecasting:** Prever valores futuros a partir de dados passados.\n",
        "- **Time-series Classification:** Classificar padr√µes em s√©ries temporais.\n",
        "\n",
        "---\n",
        "\n",
        "### üé≤ 6. Geradores, Embeddings e Modelos Multimodais\n",
        "- **Embedding:** Representa√ß√£o densa e cont√≠nua de dados categ√≥ricos (palavras, classes, etc.) Representacao matematicas(Vetores) para objetos/itens reais - Converte imagens em Grey Scala e depois em vetores - Nao existe quantidade fix the classes\n",
        "- **CLIP:** Treina modelos de texto e imagem juntos, ligando frases a imagens.\n",
        "- **Contrastive Learning:** Aproxima pares relacionados no espa√ßo de embedding e afasta os n√£o-relacionados.\n",
        "- **Maximally-Activating Images:** Imagens que mais ativam um neur√¥nio, ajudam na interpreta√ß√£o.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è 7. √âtica, Interpreta√ß√£o e Seguran√ßa\n",
        "- **Adversarial Attacks:** Pequenas perturba√ß√µes que enganam o modelo (ex: imagem de panda classificada como gibbon, fitas na placa de STOP para enganar o carro, ADD Ruidos).\n",
        "- **Pretext Task:** Tarefa auxiliar usada para treinar um modelo de forma auto-supervisionada.\n",
        "- **Instrumental Convergence:** Modelos poderosos podem buscar objetivos perigosos mesmo com metas neutras.\n",
        "- **Orthogonality Thesis:** Intelig√™ncia e objetivos s√£o independentes.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÖ PLANO DE ESTUDOS ‚Äì 3 SEMANAS\n",
        "\n",
        "### **Semana 1: Fundamentos + Arquiteturas**\n",
        "- üìå Segunda: Forward/backpropagation + fun√ß√µes de ativa√ß√£o  \n",
        "  üîó V√≠deo: [Neural Networks from Scratch (3Blue1Brown)](https://youtu.be/aircAruvnKk)\n",
        "- üìå Ter√ßa: Redes densas + regulariza√ß√£o + early stopping  \n",
        "  üîó V√≠deo: [Overfitting and Regularization](https://youtu.be/NmIYN0CxyG0)\n",
        "- üìå Quarta: CNNs e Pooling  \n",
        "  üîó V√≠deo: [CNNs ‚Äì Simplified](https://youtu.be/YRhxdVk_sIs)\n",
        "- üìå Quinta: Autoencoders e VAEs  \n",
        "  üîó V√≠deo: [Variational Autoencoder Explained](https://youtu.be/W8XF3ME8TnY)\n",
        "- üìå Sexta: GANs  \n",
        "  üîó V√≠deo: [GANs in 5 Minutes](https://youtu.be/8L11aMN5KY8)\n",
        "\n",
        "---\n",
        "\n",
        "### **Semana 2: Otimiza√ß√£o + Classifica√ß√£o + RNN**\n",
        "- üìå Segunda: GD, SGD, Momentum, RMSProp  \n",
        "  üîó V√≠deo: [Optimizers Explained Simply](https://youtu.be/nhqo0u1a6fw)\n",
        "- üìå Ter√ßa: Softmax, cross-entropy, avalia√ß√£o  \n",
        "  üîó V√≠deo: [Loss Functions Explained](https://youtu.be/MX8xx3jMnJo)\n",
        "- üìå Quarta: Ensemble methods  \n",
        "  üîó V√≠deo: [Ensemble Learning (Bagging/Boosting)](https://youtu.be/jrWHhZyH2cU)\n",
        "- üìå Quinta: RNN e LSTM  \n",
        "  üîó V√≠deo: [LSTM Explained](https://youtu.be/8HyCNIVRbSU)\n",
        "- üìå Sexta: Time series + Teacher forcing  \n",
        "  üîó V√≠deo: [Time Series Forecasting with RNN](https://youtu.be/y4WW3OVumVI)\n",
        "\n",
        "---\n",
        "\n",
        "### **Semana 3: Gera√ß√£o + CLIP + √âtica**\n",
        "- üìå Segunda: Embeddings e CLIP  \n",
        "  üîó V√≠deo: [CLIP Explained Visually](https://youtu.be/UuRGO7mGJBA)\n",
        "- üìå Ter√ßa: Contrastive Learning  \n",
        "  üîó V√≠deo: [Contrastive Learning ‚Äì Explained](https://youtu.be/F4tHL8reNCs)\n",
        "- üìå Quarta: Transformers  \n",
        "  üîó V√≠deo: [Transformers from Scratch ‚Äì CodeEmporium](https://youtu.be/U0s0f995w14)\n",
        "- üìå Quinta: Adversarial attacks e seguran√ßa  \n",
        "  üîó V√≠deo: [How to Fool AI](https://youtu.be/Br1wK6WmkiY)\n",
        "- üìå Sexta: √âtica e discuss√µes filos√≥ficas sobre IA  \n",
        "  üîó V√≠deo: [The AI Alignment Problem](https://youtu.be/xoVJKj8lcNQ)\n",
        "\n",
        "---\n",
        "\n",
        "Se quiser, posso montar flashcards interativos, simulados por tema ou mesmo PDFs prontos para revis√£o r√°pida. Quer que eu te ajude com isso tamb√©m?"
      ],
      "metadata": {
        "id": "1-6aET7zgyeb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQVRMgZncGmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}