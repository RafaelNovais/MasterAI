{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbhk0VjZkEHQjtGcIVETjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/AgenteExams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic Explanatory Answers to CT5130 / CT5134 Exam Parts (Versão Expandida)\n",
        "\n",
        "> **Nota:** Este guia é ilustrativo. No exame, adapte exemplos, aprofunde fórmulas e cite com precisão.\n",
        "\n",
        "---\n",
        "\n",
        "## SECTION A — Agents & Multi‑Agent Systems\n",
        "\n",
        "### Question 1 — Part (a)  *Termos‑chave com exemplos*\n",
        "| Termo | Explicação | Exemplo concreto |\n",
        "|-------|-----------|------------------|\n",
        "| **Agent** | Sistema autónomo que percebe o ambiente (sensores) e actua (actuadores) para maximizar uma função‑objectivo. | Aspirador **Roomba** adapta rota; **Siri** interpreta voz e responde. |\n",
        "| **Multi‑Agent System (MAS)** | Conjunto de agentes que interagem (cooperação ou competição) num ambiente comum; requer protocolos de coordenação. | **Enxame** de drones que fazem busca‑e‑salvamento dividindo área e partilhando achados. |\n",
        "| **Abstract Agent Architecture** | Modelo conceptual que separa percepção, decisão e acção; pode ser reactiva, deliberativa ou híbrida. | Arquitectura **Subsumption** (Brooks) de robôs insectoides; arquitectura **BDI** usada em planeadores logísticos. |\n",
        "| **Game Theory** | Ferramenta matemática para analisar escolhas interdependentes; focaliza estratégias e equilíbrios (Nash). | Leilões online, divisão de recursos em rede 5G, estratégia de preço entre rivais. |\n",
        "| **Agent Environment** | Mundo onde o agente opera, classificado p.ex. como determinístico/estocástico, totalmente/ parcialmente observável, discreto/contínuo. | Simulador **OpenAI Gym CartPole** (determinístico, totalmente observável, contínuo). |\n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (b)  *Comparações chave*\n",
        "- **Object × Agent** — Agente = o “tomador de decisões” que aprende Entidade ativa que decide ações para maximizar retorno acumulado.. Objeto/Ambiente = o “mundo” (ou alvo, tarefa) sobre o qual o agente age; gera feedback Entidade passiva-reativa que recebe ações, devolve nova observação/estado e recompensa..\n",
        "- **Agent × Expert System** —  Agentes Inteligentes Agir autonomamente num ambiente dinâmico para alcançar metas, adaptando-se a mudanças Percepção → Decisão → Ação em ciclo contínuo; interação em tempo real com o meio. Sistemas Especialistas  Reproduzir decisões de um perito humano numa área restrita por meio de regras fixas.Consulta pontual: o usuário fornece fatos ⇒ motor de inferência aplica regras ⇒ retorna conclusão.\n",
        "- **Zero‑Sum × Non‑Zero‑Sum Games** — Xadrez/Par-Impar/Odd-Even é zero‑sum sempre um ganha/perde meu lucro vem de alguem; comércio internacional/ negociacao  ambos ganham  é non‑zero‑sum (ganham ambos se trocarem).\n",
        "- **Supervised × Unsupervised Learning** — Supervisionado: classificador de e‑mails spam/ham; não‑supervisionado: clustering de clientes por perfil de compra.\n",
        "- **Dominant × Non‑Dominant Strategy** — Num leilão Vickrey licitar valor verdadeiro é dominante, Dilema do prisioneiro existe uma estrategia para sempre ter um melhor resultado sem depender da acao do oponete ; em leilão inglês não existe estratégia dominante pois depende da acao dos outros, Par/Impar Odd/Even nao tem uma estragia dominante pois o resultado sempre depende da acao do oponente Matching Pennies Agente = o “tomador de decisões” que aprende..\n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (c)  *Prisoner’s Dilemma aprofundado*\n",
        "1. **Payoffs**: Tentação T=5, Recompensa R=3, Punição P=1, Sucker S=0; exige T>R>P>S e 2R>T+S.\n",
        "2. **Escolha racional (jogo único)**: Defectar garante ≥ R.\n",
        "3. **Cooperação iterada**: Estratégia **Tit‑for‑Tat** (C -> depois replica oponent) vence se probabilidade de novo encontro ≥ 0.33.\n",
        "4. **Casos reais**: Publicação de código fonte aberto, redução de emissões de carbono, denúncia cartel.\n",
        "5. **Torneio Axelrod**: TFT superou 62 estratégias; requisitos para cooperação: ser **amigável**, **vingativo**, **perdoador** e **claro**.\n",
        "\n",
        "*(Para variantes A‑D, reduza T de 8→2 para observar aumento progressivo de cooperação; use simulação iterada de 200 rondas).*  \n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (b) (Robôs Arduino) / (Estacionamento de carro)\n",
        "| Termo | Robôs Arduino (mapeamento) | Carro autónomo (estacionar) |\n",
        "|-------|----------------------------|-----------------------------|\n",
        "| **Action** | Avançar 10 cm, rodar 15°, scan LIDAR | Ângulo do volante, aceleração, travão |\n",
        "| **MAS** | Partilha mapas SLAM em rede mesh | Carros vizinhos cooperam para ceder espaço |\n",
        "| **Utility** | –distância à porta – λ×colisões | –tempo – penalização por raspar obstáculos |\n",
        "| **Environment** | Desconhecido, luz variável, obstáculos | Pistas estreitas, pedestres imprevisíveis |\n",
        "| **Learning Agent** | Q‑learning com função de recompensa baseada em avanço limpo | DQN que aprende política de manobra a partir de câmaras |\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2 — Part (a)  *Leilões / Portal de Contratos*\n",
        "1. **English (ascendente)** – eBay; simples, transparente; vulnerável a «sniping».  \n",
        "2. **Dutch (descendente)** – Leilão de flores; rápido; medo de over‑pay pode atrasar lances.  \n",
        "3. **Vickrey** – Google Ads; licitante paga 2.º maior valor; incentiva honestidade; exige confiança no mecanismo.  \n",
        "**Recomendação**: Use **Vickrey‑Clarke‑Groves** num portal com **blockchain** para auditabilidade; agentes verificam sinal de colusão (corr. de lances) e ajustam regras.\n",
        "\n",
        "### Question 2 — Part (b)  *Emergência da cooperação*\n",
        "- **Axelrod:** TFT próspera se horizonte indefinido, baixa ruído de percepção.\n",
        "- **Nowak (jogos espaciais):** Vizinhanças fixas protegem clusters cooperativos (regra 5→8 viz. retarda invasão de defectores).\n",
        "- **Riolo (social tags):** Agentes atribuem tag; prob. cooperar ↑ se tag igual; mutações ≈ 1 % evitam colapso.\n",
        "\n",
        "### Question 2 — Part (c)  *Proposta agent‑based (smart‑grid)*\n",
        "- **Agente doméstico** monitoriza consumo, geração solar, preço spot.\n",
        "- **Learning:** DQN + Prioritized Replay aprende quando carregar bateria.\n",
        "- **Utility:** Lucro − penalização por energia comprada no pico.\n",
        "- **Métricas:** € poupados/ano, CO₂ evitado, tempo de resposta < 1 s.\n",
        "\n",
        "---\n",
        "\n",
        "## SECTION B — Reinforcement Learning\n",
        "\n",
        "### Question 3 — Part (a)  *MDPs discretos × contínuos + Curse*\n",
        "- **Discreto:** Discreto ⇢ lista-se cada estado e ação, usa-se tabelas e algoritmos exatos; ideal para problemas pequenos, contagens, jogos de tabuleiro, grades. Tudo cabe numa tabela; um Q-learning tabular aprende rapidamente a rota ótima.\n",
        "\n",
        "- **Contínuo:** estado/ação são números reais; exige aproximação funcional e otimização numérica; típico de robótica, controle de veículos, finanças quantitativas. Aqui não há como enumerar todas as combinações de, usamos uma rede neural actor-critic (p.ex. PPO) que gera diretamente um valor de aceleração e recebe gradientes para melhorar a política.\n",
        "\n",
        "- **Curse:** Dimensão d ⇒ |S|∝k^d; mitigue com **tile coding** ou **auto‑encoders**.\n",
        "\n",
        "### Question 3 — Part (b)  *Tipos de RL*\n",
        "**Baseado em valor**\n",
        "Observa a situação.\n",
        "Consulta sua tabela mental de valores.\n",
        "Escolhe a ação com o maior valor (“argmax”).\n",
        "Depois de ver a recompensa do mundo, ajusta a tabela.\n",
        "Analogia: criança que aprende o menu do restaurante: “Batata frita = 9 pontos de alegria, Brócolis = 2”. Quando chega o garçom ela pede o prato com maior pontuação.\n",
        "\n",
        "**Baseado em política**\n",
        "Observa a situação.\n",
        "Usa uma regra (muitas vezes uma rede neural) que já cospe a ação ideal — não consulta tabelas. Define o modo como o agente se comporta conjunto de regras baseados em estimulos e respostas.\n",
        "Se a ação deu certo, reforça essa regra; se deu errado, enfraquece-a.\n",
        "Analogia: dançarino profissional que, ao ouvir a música, move o corpo naturalmente — ele não compara movimentos possíveis um a um, já responde direto com um gesto fluido.\n",
        "\n",
        "**Ator-crítico**\n",
        "O ator propõe uma ação imediata.\n",
        "O crítico acompanha a cena como um juiz e diz “Boa!” ou “Meh… pode melhorar”.\n",
        "O ator ajusta seus próximos movimentos conforme esse feedback.\n",
        "Analogia: atleta com treinador ao lado: o atleta corre (ator) e o treinador (crítico) comenta “incline mais o tronco”. Assim, correções acontecem em tempo real e o aprendizado é mais rápido do que se o atleta treinasse sozinho.\n",
        "\n",
        "### Question 3 — Part (c)  *MDP vs POMDP exemplos*\n",
        "**MDP Markov - O que vou Fazer Dependendo da onde estou **\n",
        "\n",
        "- **Full:“Eu vejo tudo, logo decido direto.”**\n",
        "Robô separador de correspondência em um centro de triagem\n",
        "Estado: posição do braço robótico, ID da carta no visor, bandeja de destino vazia ou cheia.\n",
        "Observação: câmeras e leitores RFID extremamente confiáveis → o robô sempre identifica corretamente cada envelope e sabe exatamente onde está.\n",
        "Ação: mover braço para pegar, girar, soltar carta.\n",
        "Como ele vê todo o estado (não há ruído relevante), basta usar um algoritmo de MDP comum para minimizar o tempo médio por carta.\n",
        "\n",
        "- **Partial:“Vejo apenas pistas; decido enquanto estimo onde realmente estou.”** Apenas um sinal incompleto ou ruidoso ligado ao estado; ele sabe que pode estar em vários estados compatíveis com a percepção.De uma crença (distribuição de probabilidade) sobre onde ele provavelmente está.\n",
        "Poker Texas Hold’em – cartas ocultas.\n",
        "\n",
        "### Question 3 — Part (d)  *Determinístico vs Estocástico*\n",
        "- **T determinístico:** Único e previsível: sempre leva ao mesmo próximo estado Labirinto sem ruído; cada acção previsível.\n",
        "\n",
        "- **T estocástico:** Distribuição de possibilidades: a mesma ação pode levar a diferentes estados, cada um com certa probabilidade. Drone com rajadas de vento; prob. 0.8 manter direcção, 0.2 desvio.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4 — Part (a)  *Action‑value & update*\n",
        "- **Exemplo prático:** Único e previsível: sempre leva ao mesmo próximo estadoRobot aspirador decide «limpar vs recarregar». Q(baixa‑bateria, recarregar)=9.\n",
        "\n",
        "### Question 4 — Part (b)  *Exemplo 2×2 MDP*\n",
        "Estados S₀, S₁; acções A, B; parta de Q=0, mostre 2 iterações.\n",
        "\n",
        "### Question 4 — Part (c)  *Model‑free vs Model‑based*\n",
        "- **Model‑free (Q‑learning):trial-and-error** Como funciona: a rede estima Q(s,a); o agente joga, recebe recompensa, atualiza Q diretamente pela regra de Q-learning.Domínio em que se destaca: jogos de vídeo (ex.: Atari). O jogo roda milhões de quadros por hora em emulador, então coletar enormes volumes de experiência bruta é barato; não vale a pena gastar tempo aprendendo um modelo, basta trial-and-error até a rede dominar o jogo.“aprender agindo”. Ótimo quando dados são abundantes e baratos.\n",
        "\n",
        "- **Model‑based (Dyna‑Q): ** Como funciona: em vez de receber o modelo pronto, o agente aprende uma rede que prediz transição, recompensa e valor latente. Planeja com Monte-Carlo Tree Search sobre o modelo aprendido (“imaginação” interna) antes de cada lance.Domínio em que se destaca: Xadrez, Go, e jogos de tabuleiro complexos — cada partida real demanda tempo humano ou recurso de servidor, então gerar milhares de partidas simuladas dentro do próprio modelo é muito mais eficiente do que jogar todas elas no ambiente verdadeiro. “aprender a agir e a prever”. Indispensável quando cada interação real é valiosa ou arriscada, porque permite planejar no sofá antes de dar o próximo passo no mundo.\n",
        "\n",
        "\n",
        "\n",
        "### Question 4 — Part (d)  *Exploration‑Exploitation*\n",
        "O dilema surge porque explorar demais gasta tempo fazendo escolhas subótimas, enquanto explorar de menos pode prender o agente em soluções medíocres (ótimos locais). O problema é decidir, a cada passo, quando correr o risco de explorar e quanto explorar.obriga o agente a equilibrar curiosidade e ganância.\n",
        "\n",
        "- **Exploration/Explorar**:\n",
        "tentar ações pouco testadas para descobrir se são melhores.\n",
        "faz a exploração. Assim, ele dará chances iguais de escolha para todas as ações, não importando sua recompensa média.\n",
        "\n",
        "- **Exploitation/Explotar**:\n",
        "escolher as ações que já se mostraram boas, colhendo o “lucro” conhecido.\n",
        "é uma técnica que estará sempre procurando repetir a ação que já apresentou o melhor resultado – aquela que apresentou a maior recompensa – lembrando que não sabemos qual é a ação que dá a maior recompensa média dentre todas as possíveis.\n",
        "\n",
        "---\n",
        "\n",
        "### Tópicos adicionais (2023‑24 Autumn)\n",
        "- **Reward Hypothesis**: + Unificador; – Falha em objectivos qualitativos (ética).\n",
        "- **SARSA vs Q‑learning**: SARSA on‑policy (segue política actual) ⇒ seguro em navegação urbana.\n",
        "- **DQN Avanços**: *Replay*, *Target Net*, *ConvNets* extraem features de pixels.\n",
        "- **V(s) vs Q(s,a)**: **Monte Carlo Tree Search** usa V(s) em Go; controle fino de torque precisa Q(s,a).\n",
        "\n"
      ],
      "metadata": {
        "id": "FU1WqHgT8hQn"
      }
    }
  ]
}