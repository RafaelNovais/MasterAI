{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOVkIJ+4ZjLepOar7CtmPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/AgenteExams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic Explanatory Answers to CT5130 / CT5134 Exam Parts (Versão Expandida)\n",
        "\n",
        "> **Nota:** Este guia é ilustrativo. No exame, adapte exemplos, aprofunde fórmulas e cite com precisão.\n",
        "\n",
        "---\n",
        "\n",
        "## SECTION A — Agents & Multi‑Agent Systems\n",
        "\n",
        "### Question 1 — Part (a)  *Termos‑chave com exemplos*\n",
        "| Termo | Explicação | Exemplo concreto |\n",
        "|-------|-----------|------------------|\n",
        "| **Agent** | Sistema autónomo que percebe o ambiente (sensores) e actua (actuadores) para maximizar uma função‑objectivo. | Aspirador **Roomba** adapta rota; **Siri** interpreta voz e responde. |\n",
        "| **Multi‑Agent System (MAS)** | Conjunto de agentes que interagem (cooperação ou competição) num ambiente comum; requer protocolos de coordenação. | **Enxame** de drones que fazem busca‑e‑salvamento dividindo área e partilhando achados. |\n",
        "| **Abstract Agent Architecture** | Modelo conceptual que separa percepção, decisão e acção; pode ser reactiva, deliberativa ou híbrida. | Arquitectura **Subsumption** (Brooks) de robôs insectoides; arquitectura **BDI** usada em planeadores logísticos. |\n",
        "| **Game Theory** | Ferramenta matemática para analisar escolhas interdependentes; focaliza estratégias e equilíbrios (Nash). | Leilões online, divisão de recursos em rede 5G, estratégia de preço entre rivais. |\n",
        "| **Agent Environment** | Mundo onde o agente opera, classificado p.ex. como determinístico/estocástico, totalmente/ parcialmente observável, discreto/contínuo. | Simulador **OpenAI Gym CartPole** (determinístico, totalmente observável, contínuo). |\n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (b)  *Comparações chave*\n",
        "- **Object × Agent** — Objecto Java «Impressora» apenas executa métodos quando chamado; **Agente impressora** monitoriza fila e re‑ordena jobs optimizando tempo.\n",
        "- **Agent × Expert System** — Sistema especialista **MYCIN** deduz diagnóstico via regras estáticas; **Agente médico** aprende (RL) novas doses em função de sinais vitais.\n",
        "- **Zero‑Sum × Non‑Zero‑Sum Games** — Xadrez é zero‑sum; comércio internacional é non‑zero‑sum (ganham ambos se trocarem).\n",
        "- **Supervised × Unsupervised Learning** — Supervisionado: classificador de e‑mails spam/ham; não‑supervisionado: clustering de clientes por perfil de compra.\n",
        "- **Dominant × Non‑Dominant Strategy** — Num leilão Vickrey licitar valor verdadeiro é dominante; em leilão inglês não existe estratégia dominante pois depende da actuação dos outros.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (c)  *Prisoner’s Dilemma aprofundado*\n",
        "1. **Payoffs**: Tentação T=5, Recompensa R=3, Punição P=1, Sucker S=0; exige T>R>P>S e 2R>T+S.\n",
        "2. **Escolha racional (jogo único)**: Defectar garante ≥ R.\n",
        "3. **Cooperação iterada**: Estratégia **Tit‑for‑Tat** (C -> depois replica oponent) vence se probabilidade de novo encontro ≥ 0.33.\n",
        "4. **Casos reais**: Publicação de código fonte aberto, redução de emissões de carbono, denúncia cartel.\n",
        "5. **Torneio Axelrod**: TFT superou 62 estratégias; requisitos para cooperação: ser **amigável**, **vingativo**, **perdoador** e **claro**.\n",
        "\n",
        "*(Para variantes A‑D, reduza T de 8→2 para observar aumento progressivo de cooperação; use simulação iterada de 200 rondas).*  \n",
        "\n",
        "---\n",
        "\n",
        "### Question 1 — Part (b) (Robôs Arduino) / (Estacionamento de carro)\n",
        "| Termo | Robôs Arduino (mapeamento) | Carro autónomo (estacionar) |\n",
        "|-------|----------------------------|-----------------------------|\n",
        "| **Action** | Avançar 10 cm, rodar 15°, scan LIDAR | Ângulo do volante, aceleração, travão |\n",
        "| **MAS** | Partilha mapas SLAM em rede mesh | Carros vizinhos cooperam para ceder espaço |\n",
        "| **Utility** | –distância à porta – λ×colisões | –tempo – penalização por raspar obstáculos |\n",
        "| **Environment** | Desconhecido, luz variável, obstáculos | Pistas estreitas, pedestres imprevisíveis |\n",
        "| **Learning Agent** | Q‑learning com função de recompensa baseada em avanço limpo | DQN que aprende política de manobra a partir de câmaras |\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2 — Part (a)  *Leilões / Portal de Contratos*\n",
        "1. **English (ascendente)** – eBay; simples, transparente; vulnerável a «sniping».  \n",
        "2. **Dutch (descendente)** – Leilão de flores; rápido; medo de over‑pay pode atrasar lances.  \n",
        "3. **Vickrey** – Google Ads; licitante paga 2.º maior valor; incentiva honestidade; exige confiança no mecanismo.  \n",
        "**Recomendação**: Use **Vickrey‑Clarke‑Groves** num portal com **blockchain** para auditabilidade; agentes verificam sinal de colusão (corr. de lances) e ajustam regras.\n",
        "\n",
        "### Question 2 — Part (b)  *Emergência da cooperação*\n",
        "- **Axelrod:** TFT próspera se horizonte indefinido, baixa ruído de percepção.\n",
        "- **Nowak (jogos espaciais):** Vizinhanças fixas protegem clusters cooperativos (regra 5→8 viz. retarda invasão de defectores).\n",
        "- **Riolo (social tags):** Agentes atribuem tag; prob. cooperar ↑ se tag igual; mutações ≈ 1 % evitam colapso.\n",
        "\n",
        "### Question 2 — Part (c)  *Proposta agent‑based (smart‑grid)*\n",
        "- **Agente doméstico** monitoriza consumo, geração solar, preço spot.\n",
        "- **Learning:** DQN + Prioritized Replay aprende quando carregar bateria.\n",
        "- **Utility:** Lucro − penalização por energia comprada no pico.\n",
        "- **Métricas:** € poupados/ano, CO₂ evitado, tempo de resposta < 1 s.\n",
        "\n",
        "---\n",
        "\n",
        "## SECTION B — Reinforcement Learning\n",
        "\n",
        "### Question 3 — Part (a)  *MDPs discretos × contínuos + Curse*\n",
        "- **Discreto:** Gridworld 10×10, 4 acções; tabela Q 100×4.\n",
        "- **Contínuo:** Controlo de pêndulo (θ, \\dot{θ}); usa NN Q(·|w).\n",
        "- **Curse:** Dimensão d ⇒ |S|∝k^d; mitigue com **tile coding** ou **auto‑encoders**.\n",
        "\n",
        "### Question 3 — Part (b)  *Tipos de RL*\n",
        "| Tipo | Ideia | Algoritmo | Domínio exemplo |\n",
        "|------|------|-----------|-----------------|\n",
        "| Value‑based | Estimar Q(s,a) | DQN | Ataris 2600 |\n",
        "| Policy‑based | Optimizar πθ | PPO | Robô «Humanoid Walker» |\n",
        "| Actor‑Critic | Actor + Critic | A3C, SAC | Carro autônomo contínuo |\n",
        "\n",
        "### Question 3 — Part (c)  *MDP vs POMDP exemplos*\n",
        "- **Full:** Jogo «Quatro em Linha» – estado visível.\n",
        "- **Partial:** Poker Texas Hold’em – cartas ocultas.\n",
        "\n",
        "### Question 3 — Part (d)  *Determinístico vs Estocástico*\n",
        "- **T determinístico:** Labirinto sem ruído; cada acção previsível.\n",
        "- **T estocástico:** Drone com rajadas de vento; prob. 0.8 manter direcção, 0.2 desvio.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4 — Part (a)  *Action‑value & update*\n",
        "- **Exemplo prático:** Robot aspirador decide «limpar vs recarregar». Q(baixa‑bateria, recarregar)=9.\n",
        "\n",
        "### Question 4 — Part (b)  *Exemplo 2×2 MDP*\n",
        "Estados S₀, S₁; acções A, B; parta de Q=0, mostre 2 iterações.\n",
        "\n",
        "### Question 4 — Part (c)  *Model‑free vs Model‑based*\n",
        "- **Model‑free (Q‑learning):** Bom em vídeo‑jogos onde modelo desconhecido.\n",
        "- **Model‑based (Dyna‑Q):** Bom em planeamento logístico onde simulação barata.\n",
        "\n",
        "### Question 4 — Part (d)  *Exploration‑Exploitation*\n",
        "- **ϵ‑greedy decrescente**: ϵ₀=1→0.05 log‑schedule.\n",
        "- **UCB1**: a*=argmax(Q+√(2ln t/nₐ)).\n",
        "\n",
        "---\n",
        "\n",
        "### Tópicos adicionais (2023‑24 Autumn)\n",
        "- **Reward Hypothesis**: + Unificador; – Falha em objectivos qualitativos (ética).\n",
        "- **SARSA vs Q‑learning**: SARSA on‑policy (segue política actual) ⇒ seguro em navegação urbana.\n",
        "- **DQN Avanços**: *Replay*, *Target Net*, *ConvNets* extraem features de pixels.\n",
        "- **V(s) vs Q(s,a)**: **Monte Carlo Tree Search** usa V(s) em Go; controle fino de torque precisa Q(s,a).\n",
        "\n",
        "---\n",
        "\n",
        "## Dicas de Exame\n",
        "1. 2 h ⇒ ≈30 min por questão escolhida.\n",
        "2. Comece por definir termos; exemplifique.\n",
        "3. Demonstre ligação teórica–prática (ex.: PD → mudança climática).\n",
        "\n"
      ],
      "metadata": {
        "id": "FU1WqHgT8hQn"
      }
    }
  ]
}